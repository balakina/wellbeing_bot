import asyncio
import datetime
import json
import os
import re
from typing import Optional, TypedDict, Dict, Any, List

from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_gigachat.chat_models import GigaChat

print("RUNNING FILE:", __file__)
load_dotenv()


# ===================== MCP CLIENT =====================
async def get_mcp_client() -> MultiServerMCPClient:
    return MultiServerMCPClient({
        "wellbeing": {
            "transport": "streamable_http",
            "url": os.getenv("WELLBEING_MCP_URL", "http://127.0.0.1:8100/mcp/")
        }
    })


# ===================== LLM =====================
def build_llm() -> GigaChat:
    creds = os.getenv("GIGACHAT_CREDENTIALS", "").strip()
    if not creds:
        raise RuntimeError("–ù–µ—Ç GIGACHAT_CREDENTIALS –≤ .env")
    return GigaChat(
        credentials=creds,
        verify_ssl_certs=os.getenv("GIGACHAT_VERIFY_SSL", "false").lower() == "true",
        scope=os.getenv("GIGACHAT_SCOPE", "GIGACHAT_API_PERS"),
    )


# ===================== PROMPTS =====================
SYSTEM_TAGS_INTERP_REPLY = """
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ª–∏—á–Ω–æ–≥–æ –¥–Ω–µ–≤–Ω–∏–∫–∞.
–¢–µ–±–µ –¥–∞—é—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏ –∏ –æ—Ü–µ–Ω–∫—É –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è 1‚Äì5, –∞ —Ç–∞–∫–∂–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –∑–∞–ø–∏—Å–∏.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON:
{
  "tags": "—Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2",
  "interpretation": "1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "reply": "2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é",
  "question": "1 –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å"
}

–ü—Ä–∞–≤–∏–ª–∞:
- tags: 2-5 —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
- interpretation: –º—è–≥–∫–æ –æ–±—ä—è—Å–Ω–∏ —Å–º—ã—Å–ª
- reply: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ + 1 –º–∞–ª–µ–Ω—å–∫–∏–π —à–∞–≥
- question: –æ–¥–∏–Ω —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å
–ë–µ–∑ –¥–∏–∞–≥–Ω–æ–∑–æ–≤.
""".strip()

SYSTEM_SUMMARY = """
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π JSON –¥–Ω—è: –∑–∞–ø–∏—Å–∏, mood, —Ç–µ–≥–∏.
–°–≤–æ–¥–∫–∞ –≤ 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö:
- –∫–æ–ª-–≤–æ –∑–∞–ø–∏—Å–µ–π
- —Å—Ä–µ–¥–Ω–∏–π mood
- —Ç–µ–º—ã –¥–Ω—è
- 1 –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ/–ª–∞–π—Ñ—Ö–∞–∫
- 1 –∏–¥–µ—è, —á—Ç–æ –º–æ–∂–Ω–æ –æ–±—Å—É–¥–∏—Ç—å —Å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º
–ë–µ–∑ –¥–∏–∞–≥–Ω–æ–∑–æ–≤.
""".strip()

SYSTEM_SEARCH_ANSWER = """
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞ —Å –ø–æ–∏—Å–∫–æ–º –ø–æ –ø—Ä–æ—à–ª—ã–º –∑–∞–ø–∏—Å—è–º.

–¢–µ–±–µ –¥–∞—é—Ç:
- –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –∑–Ω–∞—á–∏—Ç –ø—É—Å—Ç–æ)

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏, –∏ –ù–ï –¥–æ–±–∞–≤–ª—è–π –¥–∞—Ç—ã/—Ñ–∞–∫—Ç—ã/—Å–º–µ–∂–Ω—ã–µ —Ç–µ–º—ã.
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∑–∞–ø–∏—Å–∏.
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏.

–§–æ—Ä–º–∞—Ç:
1) 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: —á—Ç–æ –Ω–∞—à–ª–æ—Å—å/–Ω–µ –Ω–∞—à–ª–æ—Å—å
2) 1‚Äì5 —Å—Ç—Ä–æ–∫: –¥–∞—Ç–∞ + –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
3) 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å
""".strip()

SYSTEM_SMALLTALK = """
–¢—ã ‚Äî –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞.
–û—Ç–≤–µ—á–∞–π –û–î–ù–ò–ú –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ.
–ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ/–±–æ–ª—Ç–æ–≤–Ω—è ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–ø–∏—Å–∞—Ç—å —Å–æ–±—ã—Ç–∏—è –∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5.
""".strip()


# ===================== HELPERS =====================
def _today_iso() -> str:
    return datetime.date.today().isoformat()

def _is_rating(text: str) -> bool:
    return text.strip() in {"1", "2", "3", "4", "5"}

def _is_exit(text: str) -> bool:
    return text.strip().lower() in {"–≤—ã—Ö–æ–¥", "exit", "quit"}

def _parse_date(text: str) -> Optional[str]:
    t = text.lower()
    m = re.search(r"\b\d{4}-\d{2}-\d{2}\b", t)
    if m:
        return m.group(0)
    if "—Å–µ–≥–æ–¥–Ω—è" in t:
        return _today_iso()
    return None

def _is_summary(text: str) -> bool:
    t = text.lower()
    return ("—Å–≤–æ–¥–∫" in t) or ("–∏—Ç–æ–≥" in t) or ("—Ä–µ–∑—é–º–µ" in t) or (_parse_date(text) is not None)

def _is_search(text: str) -> bool:
    t = text.lower().strip()
    return (
        t.startswith("–Ω–∞–π–¥–∏") or t.startswith("–ø–æ–∏—â–∏") or t.startswith("–ø–æ–∏—Å–∫")
        or "—á—Ç–æ —è –ø–∏—Å–∞–ª" in t or "—á—Ç–æ —è –ø–∏—Å–∞–ª–∞" in t
    )

def _extract_search_query_and_mode(text: str) -> tuple[str, str]:
    raw = text.strip()
    low = raw.lower()

    mode = "semantic"
    if low.startswith("–Ω–∞–π–¥–∏!") or low.startswith("–ø–æ–∏—â–∏!") or low.startswith("–ø–æ–∏—Å–∫!"):
        mode = "rerank"

    for prefix in ("–Ω–∞–π–¥–∏!", "–ø–æ–∏—â–∏!", "–ø–æ–∏—Å–∫!", "–Ω–∞–π–¥–∏", "–ø–æ–∏—â–∏", "–ø–æ–∏—Å–∫"):
        if low.startswith(prefix):
            q = raw[len(prefix):].strip(" :‚Äî-")
            return q, mode

    return raw, mode

def _is_paths(text: str) -> bool:
    return text.strip().lower() in {"paths", "debug", "—Å—Ç–∞—Ç—É—Å", "status"}

def _is_reindex(text: str) -> bool:
    return text.strip().lower() in {"reindex", "—Ä–µ–∏–Ω–¥–µ–∫—Å", "–ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å", "–ø–µ—Ä–µ—Å–æ–±–µ—Ä–∏", "–ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å"}

GREETINGS = {"–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–∏–≤–µ—Ç–∏–∫", "–ø—Ä–∏–≤–µ—Ç–∏–∫–∏", "—Ö–∞–π", "hello", "hi", "–π–æ", "–∑–¥–∞—Ä–æ–≤–∞", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä"}
FEELINGS_MARKERS = ("–≥—Ä—É—Å—Ç", "—Ç—Ä–µ–≤–æ–∂", "—Å—Ç—Ä–∞—à", "–±–æ—é", "–≤–æ–ª–Ω—É—é", "–ø–µ—Ä–µ–∂–∏–≤–∞", "–ø–ª–æ—Ö–æ", "–æ–¥–∏–Ω–æ–∫–æ", "–∑–ª—é", "–æ–±–∏–¥", "—É—Å—Ç–∞–ª", "–≤—ã–≥–æ—Ä", "–ø–∞–Ω–∏–∫—É", "—Ä–∞–¥", "—Å—á–∞—Å—Ç–ª–∏–≤", "—Ä–∞–∑–¥—Ä–∞–∂")
SMALLTALK_SHORT = {"–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ —Ç—ã", "—á–µ –∫–∞–∫", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–æ–∫", "–ª–∞–¥–Ω–æ", "–ø–æ–Ω—è—Ç–Ω–æ", "—è—Å–Ω–æ"}

def _is_smalltalk_not_diary(text: str) -> bool:
    t = text.lower().strip()
    if any(m in t for m in FEELINGS_MARKERS):
        return False
    if t in GREETINGS:
        return True
    if len(t) <= 12 and (t in SMALLTALK_SHORT or t.endswith("–¥–µ–ª–∞") or t.endswith("—Ç—ã")):
        return True
    return False

def _extract_json_object(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, flags=re.DOTALL | re.IGNORECASE)
    if m:
        return m.group(1)
    m = re.search(r"(\{.*?\})", text, flags=re.DOTALL)
    return m.group(1) if m else None

def _safe_json_loads(js: str) -> Optional[dict]:
    if not js:
        return None
    try:
        return json.loads(js)
    except Exception:
        pass
    fixed = re.sub(r",\s*([}\]])", r"\1", js)
    if "'" in fixed and '"' not in fixed:
        fixed = fixed.replace("'", '"')
    try:
        return json.loads(fixed)
    except Exception:
        return None

def _format_hits_for_prompt(hits: List[Dict[str, Any]], max_items: int = 5) -> str:
    lines = []
    for h in hits[:max_items]:
        created_at = (h.get("created_at") or "")[:10]
        mood = h.get("mood_score")
        tags = (h.get("tags") or "").strip()
        text = (h.get("raw_text") or "").strip().replace("\n", " ")
        if len(text) > 220:
            text = text[:220] + "‚Ä¶"
        meta = []
        if created_at:
            meta.append(created_at)
        if mood is not None:
            meta.append(f"mood={mood}")
        if tags:
            meta.append(f"tags={tags}")
        meta_s = " | ".join(meta)
        lines.append(f"- [{meta_s}] {text}")
    return "\n".join(lines) if lines else ""


def _unwrap_tool_text(res: Any) -> Any:
    """
    langchain_mcp_adapters –∏–Ω–æ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –≤–∏–¥–∞:
    [{'type':'text','text':'{...json...}', ...}]
    –ü—Ä–∏–≤–µ–¥—ë–º –∫ dict/str.
    """
    if isinstance(res, list) and res and isinstance(res[0], dict) and "text" in res[0]:
        txt = res[0]["text"]
        try:
            return json.loads(txt)
        except Exception:
            return txt
    return res


# ===================== LLM =====================
async def llm_tags_interp_reply(llm: GigaChat, raw_text: str, mood_score: int,
                               similar_hits: Optional[List[Dict[str, Any]]] = None) -> Dict[str, str]:
    similar_block = ""
    if similar_hits:
        similar_block = "\n\n–ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –∑–∞–ø–∏—Å–∏:\n" + _format_hits_for_prompt(similar_hits, max_items=3)

    user_prompt = f"""–¢–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏: "{raw_text}"
Mood: {mood_score}{similar_block}

–í–µ—Ä–Ω–∏ JSON —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ. –ë–µ–∑ markdown –∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ JSON."""
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_TAGS_INTERP_REPLY},
        {"role": "user", "content": user_prompt}
    ])

    content = (resp.content or "").strip()
    js = _extract_json_object(content)
    data = _safe_json_loads(js) if js else None

    if not data:
        return {"tags": "", "interpretation": "", "reply": "–Ø –∑–∞–ø–∏—Å–∞–ª–∞ —ç—Ç–æ. –•–æ—á–µ—à—å –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—å –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å?", "question": ""}

    return {
        "tags": (data.get("tags") or "").strip(),
        "interpretation": (data.get("interpretation") or "").strip(),
        "reply": (data.get("reply") or "").strip(),
        "question": (data.get("question") or "").strip(),
    }

async def llm_daily_summary(llm: GigaChat, summary_json: dict) -> str:
    prompt = f"JSON –¥–Ω—è:\n{json.dumps(summary_json, ensure_ascii=False, indent=2)}"
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SUMMARY},
        {"role": "user", "content": prompt}
    ])
    return (resp.content or "").strip()

async def llm_answer_from_search(llm: GigaChat, query: str, hits: List[Dict[str, Any]]) -> str:
    if not hits:
        return "üîé –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π —É—Ç–æ—á–Ω–∏—Ç—å (–¥—Ä—É–≥–æ–µ —Å–ª–æ–≤–æ/—Ñ–æ—Ä–º–∞/–∫–æ–Ω—Ç–µ–∫—Å—Ç)."

    hits_block = _format_hits_for_prompt(hits, max_items=5)
    prompt = f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏:
{hits_block}
"""
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SEARCH_ANSWER},
        {"role": "user", "content": prompt}
    ])
    return (resp.content or "").strip()

async def llm_smalltalk(llm: GigaChat, user: str) -> str:
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SMALLTALK},
        {"role": "user", "content": user}
    ])
    return (resp.content or "").strip()


# ===================== STATE =====================
class DiaryState(TypedDict, total=False):
    user_input: str
    pending_text: Optional[str]
    route: str
    date: Optional[str]
    out_text: str
    search_mode: str  # semantic/rerank


def _ctx(config) -> Dict[str, Any]:
    return config["configurable"]["ctx"]


# ===================== NODES =====================
async def node_route(state: DiaryState, config) -> DiaryState:
    user = (state.get("user_input") or "").strip()
    if not user:
        return {"route": "empty", "out_text": ""}

    pending = (state.get("pending_text") or "").strip()

    if _is_exit(user):
        return {"route": "exit", "out_text": "üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!"}

    if _is_paths(user):
        return {"route": "paths"}

    if _is_reindex(user):
        return {"route": "reindex"}

    if _is_rating(user):
        if pending:
            return {"route": "save"}
        return {"route": "rating_without_text", "out_text": "–°–Ω–∞—á–∞–ª–∞ –Ω–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏, –ø–æ—Ç–æ–º –ø–æ—Å—Ç–∞–≤—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5 üôÇ"}

    if _is_summary(user):
        date = _parse_date(user) or _today_iso()
        return {"route": "summary", "date": date}

    if _is_search(user):
        q, mode = _extract_search_query_and_mode(user)
        return {"route": "search", "user_input": q, "search_mode": mode}

    if pending:
        return {"route": "need_rating", "out_text": "üòä –û—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–π 1‚Äì5:"}

    if _is_smalltalk_not_diary(user):
        return {"route": "smalltalk"}

    return {"route": "new_text"}


async def node_new_text(state: DiaryState, config) -> DiaryState:
    user = (state.get("user_input") or "").strip()
    return {"pending_text": user, "out_text": "üòä –û—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5:"}


async def node_save(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]
    log_entry_tool = ctx.get("log_entry_tool")
    semantic_only_tool = ctx.get("semantic_only_tool")  # search_semantic_only

    pending_text = (state.get("pending_text") or "").strip()
    mood = int((state.get("user_input") or "0").strip())

    if not log_entry_tool:
        return {"out_text": "‚ùå tool log_entry –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ MCP —Å–µ—Ä–≤–µ—Ä–µ.", "pending_text": None}

    # –ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ (FAISS)
    similar_hits: List[Dict[str, Any]] = []
    if semantic_only_tool and pending_text:
        try:
            similar_hits = await semantic_only_tool.ainvoke({"query": pending_text, "top_k": 15})
            similar_hits = similar_hits[:3]
        except Exception:
            similar_hits = []

    gen = await llm_tags_interp_reply(llm, pending_text, mood, similar_hits=similar_hits)
    tags, interp, reply, question = gen["tags"], gen["interpretation"], gen["reply"], gen["question"]

    await log_entry_tool.ainvoke({
        "raw_text": pending_text,
        "mood_score": mood,
        "tags": tags,
        "interpretation": interp
    })

    out_parts = []
    if reply:
        out_parts.append(f"ü§ñ {reply}".strip())
    if question:
        out_parts.append(f"‚ùì {question}".strip())
    if tags:
        out_parts.append(f"üè∑Ô∏è {tags}".strip())

    return {"out_text": "\n\n".join(out_parts).strip(), "pending_text": None}


async def node_summary(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]
    summary_tool = ctx.get("summary_tool")

    date = (state.get("date") or "").strip() or _today_iso()
    if not summary_tool:
        return {"out_text": "‚ùå tool get_daily_summary –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ MCP —Å–µ—Ä–≤–µ—Ä–µ."}

    summary = await summary_tool.ainvoke({"date": date})
    summary = _unwrap_tool_text(summary)
    text = await llm_daily_summary(llm, summary)
    return {"out_text": f"üìä –°–≤–æ–¥–∫–∞ –∑–∞ {date}:\n\n{text}"}


async def node_search(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]

    semantic_only_tool = ctx.get("semantic_only_tool")
    rerank_tool = ctx.get("rerank_tool")

    query = (state.get("user_input") or "").strip()
    mode = (state.get("search_mode") or "semantic").strip()

    if mode == "rerank" and rerank_tool:
        hits = await rerank_tool.ainvoke({"query": query, "top_k": 30, "top_n": 5})
    else:
        if not semantic_only_tool:
            return {"out_text": "‚ùå tool search_semantic_only –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ."}
        hits = await semantic_only_tool.ainvoke({"query": query, "top_k": 20})

    hits = _unwrap_tool_text(hits)
    answer = await llm_answer_from_search(llm, query, hits if isinstance(hits, list) else [])
    return {"out_text": answer}


async def node_smalltalk(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]
    user = (state.get("user_input") or "").strip()
    text = await llm_smalltalk(llm, user)
    return {"out_text": f"ü§ñ {text}\n\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –Ω–∞–ø–∏—à–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ/—á—Ç–æ —á—É–≤—Å—Ç–≤—É–µ—à—å, –∏ –æ—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5."}


async def node_paths(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    debug_tool = ctx.get("debug_tool")
    if not debug_tool:
        return {"out_text": "‚ùå debug_paths tool –Ω–µ –Ω–∞–π–¥–µ–Ω."}
    res = await debug_tool.ainvoke({})
    res = _unwrap_tool_text(res)
    return {"out_text": "DEBUG PATHS:\n" + json.dumps(res, ensure_ascii=False, indent=2)}


async def node_reindex(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    rebuild_tool = ctx.get("rebuild_tool")
    if not rebuild_tool:
        return {"out_text": "‚ùå rebuild_faiss_from_db tool –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ."}
    res = await rebuild_tool.ainvoke({"batch_size": 256})
    res = _unwrap_tool_text(res)
    return {"out_text": "‚úÖ Reindex done:\n" + json.dumps(res, ensure_ascii=False, indent=2)}


def route_to_next(state: DiaryState) -> str:
    return state.get("route", "new_text")


def build_graph():
    g = StateGraph(DiaryState)
    g.add_node("route", node_route)
    g.add_node("new_text", node_new_text)
    g.add_node("save", node_save)
    g.add_node("summary", node_summary)
    g.add_node("search", node_search)
    g.add_node("smalltalk", node_smalltalk)
    g.add_node("paths", node_paths)
    g.add_node("reindex", node_reindex)

    g.set_entry_point("route")
    g.add_conditional_edges(
        "route",
        route_to_next,
        {
            "empty": END,
            "exit": END,
            "paths": "paths",
            "reindex": "reindex",
            "new_text": "new_text",
            "save": "save",
            "need_rating": END,
            "rating_without_text": END,
            "summary": "summary",
            "search": "search",
            "smalltalk": "smalltalk",
        }
    )
    g.add_edge("new_text", END)
    g.add_edge("save", END)
    g.add_edge("summary", END)
    g.add_edge("search", END)
    g.add_edge("smalltalk", END)
    g.add_edge("paths", END)
    g.add_edge("reindex", END)
    return g.compile()


# ===================== MAIN =====================
async def main():
    print("üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MCP —Å–µ—Ä–≤–µ—Ä—É...")
    mcp_client = await get_mcp_client()
    llm = build_llm()

    tools = await mcp_client.get_tools()
    print("=== TOOLS ===")
    for t in tools:
        print(t.name, type(t), "ainvoke=", hasattr(t, "ainvoke"))
    print("=============")

    log_entry_tool = next((t for t in tools if t.name == "log_entry"), None)
    summary_tool = next((t for t in tools if t.name == "get_daily_summary"), None)
    semantic_only_tool = next((t for t in tools if t.name == "search_semantic_only"), None)
    rerank_tool = next((t for t in tools if t.name == "search_with_rerank"), None)

    debug_tool = next((t for t in tools if t.name == "debug_paths"), None)
    rebuild_tool = next((t for t in tools if t.name == "rebuild_faiss_from_db"), None)

    if not log_entry_tool:
        print("‚ùå log_entry tool –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å —Å–µ—Ä–≤–µ—Ä –∏ –ø—É—Ç—å /mcp")
        return
    if not semantic_only_tool:
        print("‚ùå search_semantic_only tool –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å —Å–µ—Ä–≤–µ—Ä.")
        return

    ctx = {
        "llm": llm,
        "log_entry_tool": log_entry_tool,
        "summary_tool": summary_tool,
        "semantic_only_tool": semantic_only_tool,
        "rerank_tool": rerank_tool,
        "debug_tool": debug_tool,
        "rebuild_tool": rebuild_tool,
    }

    graph = build_graph()

    print("‚úÖ Wellbeing-–¥–Ω–µ–≤–Ω–∏–∫ (LangGraph) –≥–æ—Ç–æ–≤!")
    print("–ö–æ–º–∞–Ω–¥—ã:")
    print(" - –ù–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏ ‚Üí –ø–æ—Ç–æ–º –æ—Ü–µ–Ω–∏ 1‚Äì5")
    print(" - '—Å–≤–æ–¥–∫–∞' / '–∏—Ç–æ–≥' / –¥–∞—Ç–∞ YYYY-MM-DD / '—Å–µ–≥–æ–¥–Ω—è' ‚Üí —Å–≤–æ–¥–∫–∞ –¥–Ω—è")
    print(" - '–Ω–∞–π–¥–∏ ...' ‚Üí —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (FAISS)")
    print(" - '–Ω–∞–π–¥–∏! ...' ‚Üí —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + rerank")
    print(" - 'paths' ‚Üí –ø–æ–∫–∞–∑–∞—Ç—å –ø—É—Ç–∏ + faiss_ntotal")
    print(" - 'reindex' ‚Üí –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å FAISS –∏–∑ SQLite")
    print(" - '–≤—ã—Ö–æ–¥' ‚Üí –≤—ã–π—Ç–∏")

    state: DiaryState = {"pending_text": None}

    while True:
        user = input("\n–¢—ã: ").strip()
        state["user_input"] = user

        new_state = await graph.ainvoke(state, config={"configurable": {"ctx": ctx}})
        out = (new_state.get("out_text") or "").strip()
        if out:
            print("\n" + out)

        state.update(new_state)

        if new_state.get("route") == "exit":
            break


if __name__ == "__main__":
    asyncio.run(main())
