# agent_core.py
import datetime
import json
import os
import re
import asyncio
from typing import Optional, TypedDict, Dict, Any, List, Tuple

"""Core agent logic (LangGraph) for the wellbeing diary.

This file is intentionally *LLM-side only*:
- The MCP server stays deterministic (SQLite/FTS/FAISS)
- The agent uses the LLM for UX (tags/interpretation/reply/summary)

Improvements made:
- Graceful degradation when optional MCP tools are absent (semantic/rerank/report)
- Better safety around malformed tool responses
- Better routing UX (empty `–Ω–∞–π–¥–∏` query -> prompt)
"""

from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_gigachat.chat_models import GigaChat
from langchain_core.tools.base import ToolException

load_dotenv()

DEBUG_LLM = os.getenv("WELLBEING_DEBUG_LLM", "false").lower() == "true"

# ---------- LLM ----------
SYSTEM_TAGS_INTERP_REPLY = """
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ª–∏—á–Ω–æ–≥–æ –¥–Ω–µ–≤–Ω–∏–∫–∞.

–¢–µ–±–µ –¥–∞—é—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏ –∏ –æ—Ü–µ–Ω–∫—É –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è 1‚Äì5 (–∏ –∏–Ω–æ–≥–¥–∞ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –∑–∞–ø–∏—Å–∏).
–ù—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–≥–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é, –æ—Ç–≤–µ—Ç –∏ –≤–æ–ø—Ä–æ—Å.

–û–¢–í–ï–¢ –î–û–õ–ñ–ï–ù –ë–´–¢–¨ –°–¢–†–û–ì–û –í –§–û–†–ú–ê–¢–ï JSON (–æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç):
{
  "tags": "—Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2",
  "interpretation": "1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "reply": "2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é",
  "question": "1 –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å"
}

–ñ–Å–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ. –ù–∏–∫–∞–∫–æ–≥–æ markdown.
- –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî —Å—Ç—Ä–æ–∫–∏.
- tags: 2-5 —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.
- –ë–µ–∑ –¥–∏–∞–≥–Ω–æ–∑–æ–≤.
""".strip()

SYSTEM_SUMMARY = """
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π JSON –¥–Ω—è: –∑–∞–ø–∏—Å–∏, mood, —Ç–µ–≥–∏.
–°–≤–æ–¥–∫–∞ –≤ 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö:
- –∫–æ–ª-–≤–æ –∑–∞–ø–∏—Å–µ–π
- —Å—Ä–µ–¥–Ω–∏–π mood
- —Ç–µ–º—ã –¥–Ω—è
- 1 –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ/–ª–∞–π—Ñ—Ö–∞–∫
- 1 –∏–¥–µ—è, —á—Ç–æ –º–æ–∂–Ω–æ –æ–±—Å—É–¥–∏—Ç—å —Å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º
–ë–µ–∑ –¥–∏–∞–≥–Ω–æ–∑–æ–≤.
""".strip()

SYSTEM_SEARCH_ANSWER = """
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞ —Å –ø–æ–∏—Å–∫–æ–º –ø–æ –ø—Ä–æ—à–ª—ã–º –∑–∞–ø–∏—Å—è–º.

–¢–µ–±–µ –¥–∞—é—Ç:
- –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –∑–Ω–∞—á–∏—Ç –ø—É—Å—Ç–æ)

–ñ–Å–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Å–º–µ–∂–Ω—ã–µ —Ç–µ–º—ã. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–∞—Ç—ã/—Ñ–∞–∫—Ç—ã.
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∑–∞–ø–∏—Å–∏.
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏.
- –î–æ–±–∞–≤–ª—è–π —Å–∏–Ω–æ–Ω–∏–º—ã/—Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä —Å–ª—ë–∑—ã -> –ø–ª–∞–∫–∞–ª–∞, —Ä–∞—Å–ø–ª–∞–∫–∞–ª–∞—Å—å, —Ä—ã–¥–∞–ª–∞, —Ä–µ–≤–µ–ª–∞)
- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å 1-2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º ("—è –ø–ª–∞–∫–∞–ª–∞", "–º–Ω–µ –±—ã–ª–æ –≥—Ä—É—Å—Ç–Ω–æ")
- –î–∞—Ç—É –≤—ã–≤–æ–¥–∏ –°–¢–†–û–ì–û –∫–∞–∫ YYYY-MM-DD (–∏–∑ created_at). –ù–ï –º–µ–Ω—è–π —Ñ–æ—Ä–º–∞—Ç.

–§–æ—Ä–º–∞—Ç:
1) 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: —á—Ç–æ –Ω–∞—à–ª–æ—Å—å/–Ω–µ –Ω–∞—à–ª–æ—Å—å
2) 1‚Äì5 —Å—Ç—Ä–æ–∫: YYYY-MM-DD ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–∏–∑ raw_text)
3) 1 –∫–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –¢–û–õ–¨–ö–û –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ (–¥—Ä—É–≥–æ–µ —Å–ª–æ–≤–æ/—Ñ–æ—Ä–º–∞/–∫–æ–Ω—Ç–µ–∫—Å—Ç/–ø–µ—Ä–∏–æ–¥)
""".strip()

SYSTEM_SMALLTALK = """
–¢—ã ‚Äî –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞.
–û—Ç–≤–µ—á–∞–π –û–î–ù–ò–ú –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ.
–ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ/–±–æ–ª—Ç–æ–≤–Ω—è ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–ø–∏—Å–∞—Ç—å —Å–æ–±—ã—Ç–∏—è –∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5.
""".strip()


def _norm_url(u: str) -> str:
    u = (u or "").strip()
    if not u:
        return "http://127.0.0.1:8100/mcp"
    return u.rstrip("/")


async def get_mcp_client() -> MultiServerMCPClient:
    base = _norm_url(os.getenv("WELLBEING_MCP_URL", "http://127.0.0.1:8100/mcp"))
    return MultiServerMCPClient({
        "wellbeing": {"transport": "streamable_http", "url": base + "/"}
    })


def build_llm() -> GigaChat:
    creds = os.getenv("GIGACHAT_CREDENTIALS", "").strip()
    if not creds:
        raise RuntimeError("–ù–µ—Ç GIGACHAT_CREDENTIALS –≤ .env")
    return GigaChat(
        credentials=creds,
        verify_ssl_certs=os.getenv("GIGACHAT_VERIFY_SSL", "false").lower() == "true",
        scope=os.getenv("GIGACHAT_SCOPE", "GIGACHAT_API_PERS"),
    )


def _today_iso() -> str:
    return datetime.date.today().isoformat()


def _is_rating(text: str) -> bool:
    return text.strip() in {"1", "2", "3", "4", "5"}


def _is_exit(text: str) -> bool:
    return text.strip().lower() in {"–≤—ã—Ö–æ–¥", "exit", "quit"}


def _parse_date_command(text: str) -> Optional[str]:
    t = text.lower().strip()
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}", t):
        return t
    if t in {"—Å–µ–≥–æ–¥–Ω—è", "—Å–≤–æ–¥–∫–∞ —Å–µ–≥–æ–¥–Ω—è", "–∏—Ç–æ–≥ —Å–µ–≥–æ–¥–Ω—è"}:
        return _today_iso()
    return None


def _is_summary(text: str) -> bool:
    t = text.lower().strip()
    return t.startswith(("—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥", "—Ä–µ–∑—é–º–µ")) or (_parse_date_command(text) is not None)


def _is_paths(text: str) -> bool:
    return text.strip().lower() in {"paths", "debug", "—Å—Ç–∞—Ç—É—Å", "status"}


def _is_reindex(text: str) -> bool:
    return text.strip().lower() in {"reindex", "—Ä–µ–∏–Ω–¥–µ–∫—Å", "–ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å", "–ø–µ—Ä–µ—Å–æ–±–µ—Ä–∏", "–ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å"}


def _is_report(text: str) -> bool:
    t = text.strip().lower()
    return t in {"report", "–æ—Ç—á–µ—Ç", "–æ—Ç—á—ë—Ç", "–≥—Ä–∞—Ñ–∏–∫", "plot"} or t.startswith(("report ", "–æ—Ç—á–µ—Ç ", "–æ—Ç—á—ë—Ç ", "–≥—Ä–∞—Ñ–∏–∫ ", "plot "))


def _is_find_cmd(text: str) -> bool:
    t = text.lower().strip()
    return t.startswith(("–Ω–∞–π–¥–∏", "–ø–æ–∏—â–∏", "–ø–æ–∏—Å–∫")) or ("—á—Ç–æ —è –ø–∏—Å–∞–ª" in t) or ("—á—Ç–æ —è –ø–∏—Å–∞–ª–∞" in t)


def _extract_find_query_and_mode(text: str) -> Tuple[str, str]:
    raw = text.strip()
    low = raw.lower()

    if ("—á—Ç–æ —è –ø–∏—Å–∞–ª" in low) or ("—á—Ç–æ —è –ø–∏—Å–∞–ª–∞" in low):
        return raw, "rerank"

    mode = "word"
    if low.startswith(("–Ω–∞–π–¥–∏!", "–ø–æ–∏—â–∏!", "–ø–æ–∏—Å–∫!")):
        mode = "rerank"

    for prefix in ("–Ω–∞–π–¥–∏!", "–ø–æ–∏—â–∏!", "–ø–æ–∏—Å–∫!", "–Ω–∞–π–¥–∏", "–ø–æ–∏—â–∏", "–ø–æ–∏—Å–∫"):
        if low.startswith(prefix):
            q = raw[len(prefix):].strip(" :‚Äî-")
            return q, mode

    return raw, mode


GREETINGS = {"–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–∏–≤–µ—Ç–∏–∫", "–ø—Ä–∏–≤–µ—Ç–∏–∫–∏", "—Ö–∞–π", "hello", "hi", "–π–æ", "–∑–¥–∞—Ä–æ–≤–∞", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä"}
FEELINGS_MARKERS = ("–≥—Ä—É—Å—Ç", "—Ç—Ä–µ–≤–æ–∂", "—Å—Ç—Ä–∞—à", "–±–æ—é", "–≤–æ–ª–Ω—É—é", "–ø–µ—Ä–µ–∂–∏–≤–∞", "–ø–ª–æ—Ö–æ", "–æ–¥–∏–Ω–æ–∫–æ", "–∑–ª—é", "–æ–±–∏–¥", "—É—Å—Ç–∞–ª", "–≤—ã–≥–æ—Ä", "–ø–∞–Ω–∏–∫—É", "—Ä–∞–¥", "—Å—á–∞—Å—Ç–ª–∏–≤", "—Ä–∞–∑–¥—Ä–∞–∂")
SMALLTALK_SHORT = {"–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ —Ç—ã", "—á–µ –∫–∞–∫", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–æ–∫", "–ª–∞–¥–Ω–æ", "–ø–æ–Ω—è—Ç–Ω–æ", "—è—Å–Ω–æ"}


def _is_smalltalk_not_diary(text: str) -> bool:
    t = text.lower().strip()
    if any(m in t for m in FEELINGS_MARKERS):
        return False
    if t in GREETINGS:
        return True
    if len(t) <= 12 and (t in SMALLTALK_SHORT or t.endswith("–¥–µ–ª–∞") or t.endswith("—Ç—ã")):
        return True
    return False


def _extract_json_object(text: str) -> Optional[str]:
    """–î–æ—Å—Ç–∞—ë–º JSON-–æ–±—ä–µ–∫—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤ —Ç–µ–∫—Å—Ç)."""
    if not text:
        return None

    # –í—ã—Ä–µ–∂–µ–º markdown fence –µ—Å–ª–∏ –µ—Å—Ç—å
    m = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, flags=re.DOTALL | re.IGNORECASE)
    if m:
        return m.group(1)

    # –ò–Ω–∞—á–µ: –æ—Ç –ø–µ—Ä–≤–æ–π { –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π }
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        return text[start:end + 1]

    return None


def _safe_json_loads(js: str) -> Optional[dict]:
    if not js:
        return None
    try:
        return json.loads(js)
    except Exception:
        pass

    # –ª—ë–≥–∫–∞—è ‚Äú–ø–æ—á–∏–Ω–∫–∞‚Äù: —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—è—Ç—ã–µ + –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ (—á–∞—Å—Ç—ã–π –∫–æ—Å—è–∫)
    fixed = re.sub(r",\s*([}\]])", r"\1", js)

    # –∑–∞–º–µ–Ω–∏–º ¬´—É–º–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏¬ª –Ω–∞ –æ–±—ã—á–Ω—ã–µ
    fixed = fixed.replace("‚Äú", '"').replace("‚Äù", '"').replace("‚Äô", "'").replace("‚Äò", "'")

    # –µ—Å–ª–∏ –≤—Å—ë –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã—Ö ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –¥–≤–æ–π–Ω—ã–µ
    if "'" in fixed and '"' not in fixed:
        fixed = fixed.replace("'", '"')

    try:
        return json.loads(fixed)
    except Exception:
        return None


def _validate_llm_json(data: dict) -> bool:
    if not isinstance(data, dict):
        return False
    for k in ("tags", "interpretation", "reply", "question"):
        if k not in data:
            return False
        if not isinstance(data.get(k), str):
            return False
    # —Ç–µ–≥–∏ –Ω–µ –æ–±—è–∑–∞–Ω—ã –±—ã—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–º–∏, –Ω–æ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏
    if not data.get("tags", "").strip():
        return False
    if not data.get("reply", "").strip():
        return False
    return True


def _format_hits_for_prompt(hits: List[Dict[str, Any]], max_items: int = 5) -> str:
    lines = []
    for h in hits[:max_items]:
        created_at = (h.get("created_at") or "")[:10]
        mood = h.get("mood_score")
        tags = (h.get("tags") or "").strip()
        text = (h.get("raw_text") or "").strip().replace("\n", " ")
        if len(text) > 220:
            text = text[:220] + "‚Ä¶"
        meta = []
        if created_at:
            meta.append(created_at)
        if mood is not None:
            meta.append(f"mood={mood}")
        if tags:
            meta.append(f"tags={tags}")
        meta_s = " | ".join(meta)
        lines.append(f"- [{meta_s}] {text}")
    return "\n".join(lines) if lines else ""


def _unwrap_tool_text(res: Any) -> Any:
    """Unwrap common MCP tool return formats.

langchain-mcp-adapters often returns a list of content blocks like:
[{"type":"text","text":"...json..."}]
or a raw python object.
"""
    if isinstance(res, list) and res:
        first = res[0]
        if isinstance(first, dict) and "text" in first:
            txt = first.get("text") or ""
            txt = txt.strip()
            if not txt:
                return ""
            try:
                return json.loads(txt)
            except Exception:
                return txt
    return res


async def llm_tags_interp_reply(
    llm: GigaChat,
    raw_text: str,
    mood_score: int,
    similar_hits: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, str]:
    similar_block = ""
    if similar_hits:
        similar_block = "\n\n–ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –∑–∞–ø–∏—Å–∏:\n" + _format_hits_for_prompt(similar_hits, max_items=3)

    user_prompt = f"""–¢–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏: "{raw_text}"
Mood: {mood_score}{similar_block}

–í–µ—Ä–Ω–∏ JSON —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ. –ù–∏–∫–∞–∫–æ–≥–æ markdown. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ JSON.
–ï—Å–ª–∏ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî —Å–æ–∫—Ä–∞—Ç–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –ø–æ–ª—è.
"""

    # 2 –ø–æ–ø—ã—Ç–∫–∏ ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –¥–ª—è structured output
    last_content = ""
    for attempt in (1, 2):
        resp = await llm.ainvoke([
            {"role": "system", "content": SYSTEM_TAGS_INTERP_REPLY},
            {"role": "user", "content": user_prompt if attempt == 1 else (user_prompt + "\n\n–ï–©–Å –†–ê–ó: –í–ï–†–ù–ò –¢–û–õ–¨–ö–û JSON, –û–î–ò–ù –û–ë–™–ï–ö–¢.")}
        ])

        content = (resp.content or "").strip()
        last_content = content

        if DEBUG_LLM:
            print(f"\n=== LLM TAGS RAW (attempt {attempt}) ===\n{content}\n=== /LLM TAGS RAW ===\n")

        js = _extract_json_object(content)
        data = _safe_json_loads(js) if js else None

        if data and _validate_llm_json(data):
            return {
                "tags": (data.get("tags") or "").strip(),
                "interpretation": (data.get("interpretation") or "").strip(),
                "reply": (data.get("reply") or "").strip(),
                "question": (data.get("question") or "").strip(),
            }

    # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî LLM –ù–ï –æ—Ç–¥–∞–ª –≤–∞–ª–∏–¥–Ω—ã–π JSON.
    # –í–∞–∂–Ω–æ: –Ω–µ ‚Äú–º–æ–ª—á–∏–º‚Äù –∏ –Ω–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫–∏, —á—Ç–æ–±—ã —Ç—ã –Ω–µ –¥—É–º–∞–ª–∞, —á—Ç–æ LLM –æ—Ç—Ä–∞–±–æ—Ç–∞–ª.
    raise RuntimeError("LLM did not return valid JSON. Last response:\n" + (last_content[:2000]))


async def llm_daily_summary(llm: GigaChat, summary_json: dict) -> str:
    prompt = f"JSON –¥–Ω—è:\n{json.dumps(summary_json, ensure_ascii=False, indent=2)}"
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SUMMARY},
        {"role": "user", "content": prompt}
    ])
    return (resp.content or "").strip()


async def llm_answer_from_search(llm: GigaChat, query: str, hits: List[Dict[str, Any]]) -> str:
    if not hits:
        return "üîé –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É."

    hits_block = _format_hits_for_prompt(hits, max_items=5)
    prompt = f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏:
{hits_block}
"""
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SEARCH_ANSWER},
        {"role": "user", "content": prompt}
    ])
    return (resp.content or "").strip()


async def llm_smalltalk(llm: GigaChat, user: str) -> str:
    resp = await llm.ainvoke([
        {"role": "system", "content": SYSTEM_SMALLTALK},
        {"role": "user", "content": user}
    ])
    return (resp.content or "").strip()


class DiaryState(TypedDict, total=False):
    chat_id: int
    user_input: str
    pending_text: Optional[str]
    route: str
    date: Optional[str]
    out_text: str
    search_mode: str
    find_query: str
    plot_path: Optional[str]


def _ctx(config) -> Dict[str, Any]:
    return config["configurable"]["ctx"]


# ---------- SESSION HELPERS via MCP ----------
async def _load_pending_from_server(ctx: Dict[str, Any], chat_id: int) -> Optional[str]:
    tool = ctx.get("get_session_tool")
    if not tool:
        return None
    res = await tool.ainvoke({"chat_id": int(chat_id)})
    res = _unwrap_tool_text(res)
    if isinstance(res, dict):
        p = res.get("pending_text")
        return (p or None)
    return None


async def _save_pending_to_server(ctx: Dict[str, Any], chat_id: int, pending_text: Optional[str]) -> None:
    if pending_text is None:
        tool = ctx.get("clear_session_tool")
        if tool:
            await tool.ainvoke({"chat_id": int(chat_id)})
        return
    tool = ctx.get("set_session_tool")
    if tool:
        await tool.ainvoke({"chat_id": int(chat_id), "pending_text": pending_text})


# ---------- NODES ----------
async def node_route(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    user = (state.get("user_input") or "").strip()
    chat_id = int(state.get("chat_id") or 0)

    if not user:
        return {"route": "empty", "out_text": ""}

    pending = await _load_pending_from_server(ctx, chat_id)
    state["pending_text"] = pending

    if _is_exit(user):
        await _save_pending_to_server(ctx, chat_id, None)
        return {"route": "exit", "out_text": "üëã –ü–æ–∫–∞!"}

    if _is_paths(user):
        return {"route": "paths"}

    if _is_reindex(user):
        return {"route": "reindex"}

    if _is_report(user):
        return {"route": "report"}

    if _is_rating(user):
        if pending:
            return {"route": "save"}
        return {"route": "rating_without_text", "out_text": "–°–Ω–∞—á–∞–ª–∞ –Ω–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏, –ø–æ—Ç–æ–º –ø–æ—Å—Ç–∞–≤—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5 üôÇ"}

    if _is_summary(user):
        date = _parse_date_command(user) or _today_iso()
        return {"route": "summary", "date": date}

    if _is_find_cmd(user):
        q, mode = _extract_find_query_and_mode(user)
        return {"route": "find", "find_query": q, "search_mode": mode}

    if pending:
        return {"route": "need_rating", "out_text": "üòä –û—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–π 1‚Äì5:"}

    if _is_smalltalk_not_diary(user):
        return {"route": "smalltalk"}

    return {"route": "new_text"}


async def node_new_text(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    chat_id = int(state.get("chat_id") or 0)
    user = (state.get("user_input") or "").strip()

    await _save_pending_to_server(ctx, chat_id, user)
    return {"out_text": "üòä –û—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5:"}


async def node_save(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]

    log_entry_tool = ctx.get("log_entry_tool")
    semantic_tool = ctx.get("semantic_tool")

    chat_id = int(state.get("chat_id") or 0)
    pending_text = await _load_pending_from_server(ctx, chat_id)
    mood = int((state.get("user_input") or "0").strip())

    if not pending_text:
        return {"out_text": "–°–Ω–∞—á–∞–ª–∞ –Ω–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏, –ø–æ—Ç–æ–º –ø–æ—Å—Ç–∞–≤—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5 üôÇ"}

    if not log_entry_tool:
        return {"out_text": "‚ùå tool log_entry –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ MCP —Å–µ—Ä–≤–µ—Ä–µ."}

    similar_hits: List[Dict[str, Any]] = []
    if semantic_tool and pending_text:
        try:
            similar_hits = await semantic_tool.ainvoke({"query": pending_text, "top_k": 15})
            similar_hits = _unwrap_tool_text(similar_hits)
            similar_hits = similar_hits[:3] if isinstance(similar_hits, list) else []
        except Exception:
            similar_hits = []

    try:
        gen = await llm_tags_interp_reply(llm, pending_text, mood, similar_hits=similar_hits)
    except Exception as e:
        # –í–ê–ñ–ù–û: –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å—å –∏ –Ω–µ –æ—á–∏—â–∞–µ–º pending.
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ü–∏—Ñ—Ä—É 1‚Äì5 –µ—â—ë —Ä–∞–∑, –∏ –º—ã –ø–æ–≤—Ç–æ—Ä—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.
        msg = "‚ùå LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è —Ç–µ–≥–æ–≤/–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏.\n" \
              "–û—Ç–ø—Ä–∞–≤—å —Ü–∏—Ñ—Ä—É 1‚Äì5 –µ—â—ë —Ä–∞–∑ ‚Äî —è –ø–æ–≤—Ç–æ—Ä—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."
        if DEBUG_LLM:
            msg += f"\n\nDEBUG: {type(e).__name__}: {str(e)[:400]}"
        return {"out_text": msg}

    tags, interp, reply, question = gen["tags"], gen["interpretation"], gen["reply"], gen["question"]

    await log_entry_tool.ainvoke({
        "raw_text": pending_text,
        "mood_score": mood,
        "tags": tags,
        "interpretation": interp
    })

    await _save_pending_to_server(ctx, chat_id, None)

    out_parts = []
    if reply:
        out_parts.append(f"ü§ñ {reply}".strip())
    if question:
        out_parts.append(f"‚ùì {question}".strip())
    if tags:
        out_parts.append(f"üè∑Ô∏è {tags}".strip())

    return {"out_text": "\n\n".join(out_parts).strip()}


async def node_summary(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]
    summary_tool = ctx.get("summary_tool")
    date = (state.get("date") or "").strip() or _today_iso()

    if not summary_tool:
        return {"out_text": "‚ùå tool get_daily_summary –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ MCP —Å–µ—Ä–≤–µ—Ä–µ."}

    summary = await summary_tool.ainvoke({"date": date})
    summary = _unwrap_tool_text(summary)
    text = await llm_daily_summary(llm, summary)
    return {"out_text": f"üìä –°–≤–æ–¥–∫–∞ –∑–∞ {date}:\n\n{text}"}


async def node_find(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]

    search_word_tool = ctx.get("search_word_tool")
    rerank_tool = ctx.get("rerank_tool")
    semantic_tool = ctx.get("semantic_tool")

    q = (state.get("find_query") or "").strip()
    mode = (state.get("search_mode") or "word").strip()

    if not q:
        return {"out_text": "–ù–∞–ø–∏—à–∏ –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã. –ù–∞–ø—Ä–∏–º–µ—Ä: `–Ω–∞–π–¥–∏ —Ç—Ä–µ–≤–æ–≥–∞` –∏–ª–∏ `–Ω–∞–π–¥–∏! –ø–æ–µ–∑–¥–∫–∞ –≤ –ö–∏—Ç–∞–π`"}

    try:
        if mode == "rerank":
            if rerank_tool:
                hits = await rerank_tool.ainvoke({"query": q, "top_k": 30, "top_n": 5})
            elif semantic_tool:
                hits = await semantic_tool.ainvoke({"query": q, "top_k": 20})
            else:
                hits = []
        else:
            if not search_word_tool:
                return {"out_text": "‚ùå tool search_word –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ."}
            hits = await search_word_tool.ainvoke({"query": q, "limit": 20})
    except ToolException as e:
        return {"out_text": f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}"}
    except Exception as e:
        return {"out_text": f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {repr(e)}"}

    hits = _unwrap_tool_text(hits)
    answer = await llm_answer_from_search(llm, q, hits if isinstance(hits, list) else [])
    return {"out_text": answer}


async def node_smalltalk(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    llm = ctx["llm"]
    user = (state.get("user_input") or "").strip()
    text = await llm_smalltalk(llm, user)
    return {"out_text": f"ü§ñ {text}\n\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –Ω–∞–ø–∏—à–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ/—á—Ç–æ —á—É–≤—Å—Ç–≤—É–µ—à—å, –∏ –æ—Ü–µ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ 1‚Äì5."}


async def node_paths(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    debug_tool = ctx.get("debug_tool")
    if not debug_tool:
        return {"out_text": "‚ùå debug_paths tool –Ω–µ –Ω–∞–π–¥–µ–Ω."}
    res = await debug_tool.ainvoke({})
    res = _unwrap_tool_text(res)
    return {"out_text": "DEBUG PATHS:\n" + json.dumps(res, ensure_ascii=False, indent=2)}


async def node_reindex(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    rebuild_tool = ctx.get("rebuild_tool")
    rebuild_fts_tool = ctx.get("rebuild_fts_tool")

    if not rebuild_tool:
        return {"out_text": "‚ùå rebuild_faiss_from_db tool –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ."}

    try:
        res1 = await rebuild_tool.ainvoke({"batch_size": 256})
        res1 = _unwrap_tool_text(res1)
    except ToolException as e:
        return {"out_text": f"‚ùå Reindex —É–ø–∞–ª: {e}"}
    except Exception as e:
        return {"out_text": f"‚ùå Reindex —É–ø–∞–ª: {repr(e)}"}

    res2 = None
    if rebuild_fts_tool:
        try:
            res2 = await rebuild_fts_tool.ainvoke({})
            res2 = _unwrap_tool_text(res2)
        except Exception:
            res2 = None

    out = "‚úÖ Reindex done:\n" + json.dumps(res1, ensure_ascii=False, indent=2)
    if res2 is not None:
        out += "\n\n‚úÖ FTS rebuild:\n" + json.dumps(res2, ensure_ascii=False, indent=2)
    return {"out_text": out}


async def node_report(state: DiaryState, config) -> DiaryState:
    ctx = _ctx(config)
    report_tool = ctx.get("report_tool")
    if not report_tool:
        return {"out_text": "‚ùå tool export_last_weeks_report –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ MCP —Å–µ—Ä–≤–µ—Ä–µ."}

    try:
        res = await report_tool.ainvoke({"weeks": 4, "out_dir": "reports"})
    except ToolException as e:
        return {"out_text": f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}"}
    except Exception as e:
        return {"out_text": f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {repr(e)}"}

    res = _unwrap_tool_text(res) if res is not None else {}
    plot_path = res.get("plot_path")
    date_from = res.get("date_from")
    date_to = res.get("date_to")
    total_entries = res.get("total_entries")

    lines = [f"üìà –ì—Ä–∞—Ñ–∏–∫ –≥–æ—Ç–æ–≤ ({date_from} ‚Üí {date_to})."]
    if total_entries is not None:
        lines.append(f"- –ó–∞–ø–∏—Å–µ–π: {total_entries}")
    if plot_path:
        lines.append(f"- PNG: {plot_path}")

    return {"out_text": "\n".join(lines), "plot_path": plot_path}


def route_to_next(state: DiaryState) -> str:
    return state.get("route", "new_text")


def build_graph():
    g = StateGraph(DiaryState)
    g.add_node("route", node_route)
    g.add_node("new_text", node_new_text)
    g.add_node("save", node_save)
    g.add_node("summary", node_summary)
    g.add_node("find", node_find)
    g.add_node("smalltalk", node_smalltalk)
    g.add_node("paths", node_paths)
    g.add_node("reindex", node_reindex)
    g.add_node("report", node_report)

    g.set_entry_point("route")
    g.add_conditional_edges(
        "route",
        route_to_next,
        {
            "empty": END,
            "exit": END,
            "paths": "paths",
            "reindex": "reindex",
            "report": "report",
            "new_text": "new_text",
            "save": "save",
            "need_rating": END,
            "rating_without_text": END,
            "summary": "summary",
            "find": "find",
            "smalltalk": "smalltalk",
        }
    )
    g.add_edge("new_text", END)
    g.add_edge("save", END)
    g.add_edge("summary", END)
    g.add_edge("find", END)
    g.add_edge("smalltalk", END)
    g.add_edge("paths", END)
    g.add_edge("reindex", END)
    g.add_edge("report", END)
    return g.compile()


async def init_ctx() -> Dict[str, Any]:
    mcp_client = await get_mcp_client()
    llm = build_llm()

    tools = await mcp_client.get_tools()

    def pick(name: str):
        return next((t for t in tools if t.name == name), None)

    ctx = {
        "llm": llm,
        "log_entry_tool": pick("log_entry"),
        "summary_tool": pick("get_daily_summary"),
        "search_word_tool": pick("search_word"),
        "semantic_tool": pick("search_semantic_only"),
        "rerank_tool": pick("search_with_rerank"),
        "debug_tool": pick("debug_paths"),
        "rebuild_tool": pick("rebuild_faiss_from_db"),
        "rebuild_fts_tool": pick("rebuild_fts_from_db"),
        "report_tool": pick("export_last_weeks_report"),
        "get_session_tool": pick("get_session"),
        "set_session_tool": pick("set_session"),
        "clear_session_tool": pick("clear_session"),
    }

    # Required tools (without them the bot is not usable).
    required = ["log_entry_tool", "search_word_tool", "get_session_tool", "set_session_tool", "clear_session_tool"]
    missing = [k for k in required if not ctx.get(k)]
    if missing:
        raise RuntimeError(
            "–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ MCP tools: "
            f"{missing}. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏ WELLBEING_MCP_URL –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π."
        )

    return ctx
